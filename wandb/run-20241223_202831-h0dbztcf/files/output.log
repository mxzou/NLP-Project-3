2024-12-23 20:28:32,034 - src.utils.logger - INFO - Initialized W&B run: run_20241223_202831
Loading datasets...
/Users/s3nik/Desktop/nlp-project-3/midi_caption_project/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/Users/s3nik/Desktop/nlp-project-3/midi_caption_project/venv/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.
For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.
- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.
- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.
- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.
  warnings.warn(
2024-12-23 20:28:32,468 - src.data.loader - INFO - Loaded 100 samples for train split
2024-12-23 20:28:32,732 - src.data.loader - INFO - Loaded 100 samples for val split
Loading model...
Starting training...
Epoch 1/3: 100%|████████████████████████████████████| 25/25 [03:38<00:00,  8.74s/it]
Epoch 2/3: 100%|████████████████████████████████████| 25/25 [03:51<00:00,  9.25s/it]
Epoch 3/3: 100%|████████████████████████████████████| 25/25 [03:51<00:00,  9.26s/it]
