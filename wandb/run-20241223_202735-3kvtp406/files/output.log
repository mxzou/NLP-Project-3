2024-12-23 20:27:36,373 - src.utils.logger - INFO - Initialized W&B run: run_20241223_202735
Loading datasets...
/Users/s3nik/Desktop/nlp-project-3/midi_caption_project/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/Users/s3nik/Desktop/nlp-project-3/midi_caption_project/venv/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.
For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.
- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.
- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.
- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.
  warnings.warn(
2024-12-23 20:27:36,787 - src.data.loader - INFO - Loaded 100 samples for train split
2024-12-23 20:27:37,034 - src.data.loader - INFO - Loaded 100 samples for val split
Loading model...
2024-12-23 20:27:37,620 - src.training.trainer - INFO - Model successfully moved to cpu
2024-12-23 20:27:37,913 - src.training.trainer - INFO - Using default AdamW optimizer
Starting training...
Epoch 1/3:   0%|                                             | 0/25 [00:11<?, ?it/s]
Error during training: 'MusicT5Trainer' object has no attribute 'training_step'
